color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(#xmin = as.numeric(ymd("2011-01-01")),
#xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = 5000 #as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = 5000, #as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2012-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2013-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years)
ggplot(all_years)
library(tidyverse)
library(tidyquant)
library(timetk)
library(broom)
ggplot(all_years)
View(all_years)
ggplot2(all_years)
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2012-2015.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2015-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years, aes(displ, hwy)) + geom_point()
ggplot(all_years, aes(x=date, y=cnt)) + geom_point()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2014-2015.csv")
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
ggplot(all_years)
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Visualize data and training/testing regions
bikes %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
test <- bikes %>%
filter(date >= ymd("2012-07-01"))
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Model using the augmented features
fit_lm <- lm(cnt ~ ., data = train_augmented)
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
# RMSE
sqrt(mean(fit_lm$residuals^2))
test_augmented <- test %>%
tk_augment_timeseries_signature()
test_augmented
yhat_test <- predict(fit_lm, newdata = test_augmented)
pred_test <- test %>%
add_column(yhat = yhat_test) %>%
mutate(.resid = cnt - yhat)
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
train
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
summary(fit_lm)
test_augmented
###a) Preliminary insights
table(reviews$review_scores_rating)
# install.packages("tm")
# install.packages("SnowballC")
library(tidyverse)
library(tm) #to create corpus and modify text therein.
library(SnowballC) #for "stemming."
library(rpart) #to construct a CART model.
library(rpart.plot) #to plot CART tree.
library(caret)
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
setwd("~/Google Drive/Colab Notebooks/R/6.1.airbnb")
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
###a) Preliminary insights
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
list(reviews$review_scores_rating)
head(reviews)
View(reviews)
head(list(reviews$review_scores_rating))
list(reviews$review_scores_rating)
reviews$review_scores_rating
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
# 1. Change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array of document
View(corpus)
corpus[[1]]   # individual doc
strwrap(corpus[[1]])
strwrap(corpus[[2]])
strwrap(corpus[[3]])
strwrap(corpus[[2]])
# The function tm_map applies an operation to every document in the corpus.
# In this case, the operation is 'tolower" (i.e. to lowercase).
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
strwrap(corpus[[1]]) #split a sentence
strwrap(corpus[[2]])
corpus[[1]]   # individual doc
(corpus[[1]])   # individual doc
print(corpus[[1]])   # individual doc
corpus
# The function tm_map applies an operation to every document in the corpus.
# In this case, the operation is 'tolower" (i.e. to lowercase).
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
###a) Preliminary insights
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
# 1. Change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array/collection of documents containing texts
corpus[[1]]   # individual doc
strwrap(corpus[[1]]) #strwrap: split a sentence
strwrap(corpus[[2]])
# The function tm_map applies an operation to every document in the corpus.
# In this case, the operation is 'tolower" (i.e. to lowercase).
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
# 2. Remove punctuation from the document
corpus <- tm_map(corpus, removePunctuation)
strwrap(corpus[[1]])
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# 3. Remove all stop words:
corpus = tm_map(corpus, removeWords, stopwords("english"))  # removeWords(corpus,stopwords("english"))
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")#[1:10]
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[1]])
# 4. Remove the particular word: 'airbnb'
# This list can be customized depending on the application context
#kwic(reviews$comments, pattern="airbnb")
#reviews[grepl("word",reviews$comments),"comments"] #look for comments containing particular word
strwrap(corpus[[162]])
corpus = tm_map(corpus, removeWords, c("airbnb"))
strwrap(corpus[[162]])
# 5. Now stem documents. This corresponds to removing the parts of words
# that are in some sense not necessary (e.g. 'ing' and 'ed').
corpus = tm_map(corpus, stemDocument)
# We have:
strwrap(corpus[[1]])
### c) Sparsify
# "Sparsify" the corpus and remove infrequent words.
# First, calculate the frequency of each words over all tweets.
frequencies = DocumentTermMatrix(corpus)
View(frequencies)
frequencies   # documents as the rows, terms as the columns
strwrap(corpus[[1]]) #strwrap: split a sentence
strwrap(corpus[[2]])
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
###a) Preliminary insights
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
# 1. Change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array/collection of documents containing texts
corpus[[1]]   # individual doc
strwrap(corpus[[1]]) #strwrap: split a sentence
strwrap(corpus[[2]])
strwrap(corpus[[1]]) #strwrap: split a sentence
# The function tm_map applies an operation to every document in the corpus.
# In this case, the operation is 'tolower" (i.e. to lowercase).
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
# 2. Remove punctuation from the document
corpus <- tm_map(corpus, removePunctuation)
strwrap(corpus[[1]])
# 3. Remove all stop words:
corpus = tm_map(corpus, removeWords, stopwords("english"))  # removeWords(corpus,stopwords("english"))
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[1]])
# 4. Remove the particular word: 'airbnb'
# This list can be customized depending on the application context
#kwic(reviews$comments, pattern="airbnb")
#reviews[grepl("word",reviews$comments),"comments"] #look for comments containing particular word
strwrap(corpus[[162]])
corpus = tm_map(corpus, removeWords, c("airbnb"))
strwrap(corpus[[162]])
# 5. Now stem documents. This corresponds to removing the parts of words
# that are in some sense not necessary (e.g. 'ing' and 'ed').
corpus = tm_map(corpus, stemDocument)
# We have:
strwrap(corpus[[1]])
### c) Sparsify
# "Sparsify" the corpus and remove infrequent words.
# First, calculate the frequency of each words over all tweets.
frequencies = DocumentTermMatrix(corpus) # constructs or coerces to a term-document matrix
frequencies   # documents as the rows, terms as the columns
frequencies[1]   # documents as the rows, terms as the columns
frequencies[[1]]   # documents as the rows, terms as the columns
frequencies   # documents as the rows, terms as the columns
View(frequencies)
View(frequencies)
# Words that appear at least 900 times:
findFreqTerms(frequencies, lowfreq=900)
# Only keep terms that appear in at least 1% of the tweets and
# create a list of these words as follows.
sparse = removeSparseTerms(frequencies, 0.99)  # 0.99: maximal allowed sparsity
sparse # We now have 404 terms instead of 7,040
sparse # We now have 404 terms instead of 7,040
### d) Training and Test Split
# Create a new data frame.
# Each column corresponds to each word (term),
# Each row corresponds to each document (review)
document_terms = as.data.frame(as.matrix(sparse))
View(document_terms)
str(document_terms)
View(document_terms)
# Create a new column for the dependent variable:
document_terms$positive_review = reviews$review_scores_rating >= 80
head(reviews)
document_terms
View(document_terms)
# Create a new column for the dependent variable:
document_terms$positive_review = reviews$review_scores_rating >= 80
document_terms
View(document_terms)
View(document_terms)
View(reviews)
### e) Prediction
# Construct and plot the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
prp(cart)
# install.packages("tm")
# install.packages("SnowballC")
library(tidyverse)
library(tm) #to create corpus and modify text therein.
library(SnowballC) #for "stemming."
library(rpart) #to construct a CART model.
library(rpart.plot) #to plot CART tree.
library(caret)
### e) Prediction
# Construct and plot the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
### d) Training and Test Split
# Create a new data frame.
# Each column corresponds to each word (term),
# Each row corresponds to each document (review)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms)
# Create a new column for the dependent variable, which indicates TRUE if the review is positive (score>=80).
document_terms$positive_review = reviews$review_scores_rating >= 80
head(reviews)
document_terms$
# Split training and test set to prepare for the modeling
split1 = (reviews$date < "2018-01-01")
split2 = (reviews$date >= "2018-01-01")
train = document_terms[split1,]
document_terms$
# Split training and test set to prepare for the modeling
split1 = (reviews$date < "2018-01-01")
# Split training and test set to prepare for the modeling
split1 = (reviews$date < "2018-01-01")
split2 = (reviews$date >= "2018-01-01")
train = document_terms[split1,]
test = document_terms[split2,]
### e) Prediction
# Construct and plot the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
prp(cart)
# Assess the out-of-sample performance of the CART model
predictions.cart <- predict(cart, newdata=test, type="class")
predictions.cart
matrix.cart = table(test$positive_review, predictions.cart) # confusion matrix
matrix.cart
accuracy.cart = (matrix.cart[1,1]+matrix.cart[2,2])/nrow(test)
# True Positive Rate (TPR) and False Positive Rate (FPR)
TPR.cart = (matrix.cart[2,2])/sum(matrix.cart[2,])
FPR.cart = (matrix.cart[1,2])/sum(matrix.cart[1,])
##### Baseline: Baseline model where all reviews are classified as positive
accuracy.baseline = sum(test$positive_review)/nrow(test)
TPR.baseline = 1
FPR.baseline = 1
#Summary of performance
summary.performance <- data.frame (
accuracy=round(c(accuracy.baseline,accuracy.cart),3),
TPR=round(c(TPR.baseline,TPR.cart),3),
FPR=round(c(FPR.baseline,FPR.cart),3))
summary.performance
