# Select date and count
all_years <- all_years %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(dteday, cnt) %>%
rename(datetime = dteday)
# Select date and count
all_years <- all_years %>%
select(dteday, cnt) %>%
rename(date = datetime)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2015-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2014-12-31")),
#ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
bikes %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(#xmin = as.numeric(ymd("2011-01-01")),
#xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = 5000 #as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = 5000, #as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2012-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2013-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years)
ggplot(all_years)
library(tidyverse)
library(tidyquant)
library(timetk)
library(broom)
ggplot(all_years)
View(all_years)
ggplot2(all_years)
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2012-2015.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2015-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years, aes(displ, hwy)) + geom_point()
ggplot(all_years, aes(x=date, y=cnt)) + geom_point()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2014-2015.csv")
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
ggplot(all_years)
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Visualize data and training/testing regions
bikes %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
test <- bikes %>%
filter(date >= ymd("2012-07-01"))
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Model using the augmented features
fit_lm <- lm(cnt ~ ., data = train_augmented)
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
# RMSE
sqrt(mean(fit_lm$residuals^2))
test_augmented <- test %>%
tk_augment_timeseries_signature()
test_augmented
yhat_test <- predict(fit_lm, newdata = test_augmented)
pred_test <- test %>%
add_column(yhat = yhat_test) %>%
mutate(.resid = cnt - yhat)
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
train
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
summary(fit_lm)
test_augmented
library(caret)
reviews = read.csv("airbnb-small.csv", stringsAsFactors = FALSE)
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
reviews = read.csv("data\airbnb-small.csv", stringsAsFactors = FALSE)
setwd("~/Google Drive/Colab Notebooks/R/6.text_analytics-cart")
reviews = read.csv("data\airbnb-small.csv", stringsAsFactors = FALSE)
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
summary(reviews)
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews, list(reviews$review_scores_rating), mean)
aggregate(reviews, list(review_scores_rating), mean)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
summary(reviews)
View(reviews)
View(reviews)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
View(reviews)
setwd("~/1. MIT Courses/2020-Spring/Analytics Edge/Recitation/Recitation 6")
# Install/load the required packages.
# install.packages("tm")
# install.packages("SnowballC")
library(tidyverse)
library(tm) # Will use to create corpus and modify text therein.
library(SnowballC) # Will use for "stemming."
library(rpart) # Will use to construct a CART model.
library(rpart.plot) # Will use to plot CART tree.
# Load the dataset (trump tweets posted between 6/1/15 and 3/1/17).
tweets = read.csv("trump_tweets.csv", stringsAsFactors = FALSE)
# Let us recall the structure of the dataset.
str(tweets)
# Let us also see how many tweets President Trump posted.
table(tweets$TrumpWrote)
# Next we will manipulate the text in President Trump's tweets. To this end,
# we will create a "corpus."
corpus = Corpus(VectorSource(tweets$text)) # An array of document
corpus
View(corpus)
# The tweets in the corpus are called "documents."
# corpus[1]     # subset
corpus[[1]]   # individual doc
strwrap(corpus[[1]])
strwrap(corpus[[3]])
strwrap(corpus[3])
strwrap(corpus[[3])
strwrap(corpus[[3]])
# 1. Let's change all the text to lower case.
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
# We now transform a link of the form "https://link" into "http link" to make sure http appears as a word
# We first define a new function f from gsub (substitute text):
# f is essentially the same as gsub except for the order of arguments
f <- content_transformer(function(doc, oldtext, newtext) gsub(oldtext, newtext, doc))
# We now transform a link of the form "https://link" into "http link" to make sure http appears as a word
# We first define a new function f from gsub (substitute text):
# f is essentially the same as gsub except for the order of arguments
# gsub() replaces all matches of a strong. ex) gsub(targettext, newtext, stringvector)
f <- content_transformer(function(doc, oldtext, newtext) gsub(oldtext, newtext, doc))
corpus <- tm_map(corpus, f, "https://", "http ")  # f(each doc in corpus, "https://", "http ")
strwrap(corpus[[3]])
# Last, we remove punctuation from the document
corpus <- tm_map(corpus, removePunctuation)
strwrap(corpus[[1]])
# 2. Let us remove some words. First, we remove stop words:
corpus = tm_map(corpus, removeWords, stopwords("english"))  # removeWords(corpus,stopwords("english"))
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[1]])
# Next, we remove the three particular words: realdonaldtrump, donaldtrump, donaldjtrumpjr.
# This list can be customized depending on the application context
strwrap(corpus[[4]])
corpus = tm_map(corpus, removeWords, c("realdonaldtrump", "donaldtrump", "donaldjtrumpjr"))
strwrap(corpus[[4]])
# 3. Now we stem our documents. Recall that this corresponds toremoving the parts of words
# that are in some sense not necessary (e.g. 'ing' and 'ed').
corpus = tm_map(corpus, stemDocument)
# We have:
strwrap(corpus[[1]])
# 4. Let us "sparsify" the corpus and remove infrequent words.
# First, we calculate the frequency of each words over all tweets.
frequencies = DocumentTermMatrix(corpus)
frequencies                              # documents as the rows, terms as the columns
# Let us get a feel for what words occur the most. Words that appear at least 200 times:
findFreqTerms(frequencies, lowfreq=200)
# Words that appear at least 100 times:
findFreqTerms(frequencies, lowfreq=100)
# Let us only keep terms that appear in at least 1% of the tweets. We create a list of these words as follows.
sparse = removeSparseTerms(frequencies, 0.99)  # 0.99: maximal allowed sparsity
sparse # We now have 172 terms instead of 12,093
# Lastly, we create a new column for the dependent variable:
document_terms$TrumpWrote = tweets$TrumpWrote
# 5. We first create a new data frame. Each variable corresponds to one of the 172 words, and each row corresponds to one of the tweets.
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms)
# Lastly, we create a new column for the dependent variable:
document_terms$TrumpWrote = tweets$TrumpWrote
# Training and test set.
split1 = (tweets$created_at < "2016-06-01")
str(document_terms)
# Lastly, we create a new column for the dependent variable:
document_terms$TrumpWrote = tweets$TrumpWrote
document_terms
View(document_terms)
# We have processed our data! Let us briefly construct a CART model.
head(tweets)
View(tweets)
View(tweets)
# Training and test set.
split1 = (tweets$created_at < "2016-06-01")
split2 = (tweets$created_at >= "2016-06-01")
train = document_terms[split1,]
test = document_terms[split2,]
# Constructing the logistic regression model
logreg = glm(TrumpWrote ~., data=train, family="binomial")
summary(logreg)
# Assessing the out-of-sample performance of the logistic regression model
predictions.logreg <- predict(logreg, newdata=test, type="response")
matrix.logreg = table(test$TrumpWrote, predictions.logreg > 0.5)   # threshold = 0.5
matrix.logreg    # confusion matrix
accuracy.logreg = (matrix.logreg[1,1]+matrix.logreg[2,2])/nrow(test)
TPR.logreg = (matrix.logreg[2,2])/sum(matrix.logreg[2,])
FPR.logreg = (matrix.logreg[1,2])/sum(matrix.logreg[1,])
# Constructing and plotting the CART model.
cart = rpart(TrumpWrote ~ ., data=train, method="class", cp = .003)  # classification
prp(cart)
# Assessing the out-of-sample performance of the CART model
predictions.cart <- predict(cart, newdata=test, type="class")
# Constructing and plotting the CART model.
cart = rpart(TrumpWrote ~ ., data=train, method="class", cp = .003)  # classification
prp(cart)
# Assessing the out-of-sample performance of the CART model
predictions.cart <- predict(cart, newdata=test, type="class")
matrix.cart = table(test$TrumpWrote, predictions.cart) # confusion matrix
accuracy.cart = (matrix.cart[1,1]+matrix.cart[2,2])/nrow(test)
TPR.cart = (matrix.cart[2,2])/sum(matrix.cart[2,])
FPR.cart = (matrix.cart[1,2])/sum(matrix.cart[1,])
matrix.cart
# 1. Let's change all the comments to lower case.
corpus = tm_map(reviews$comments, tolower)
strwrap(corpus[[1]])
setwd("~/Google Drive/Colab Notebooks/R/6.text_analytics-cart")
library(caret)
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
#a) Preliminary insights
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
# 1. Let's change all the comments to lower case.
corpus = tm_map(reviews$comments, tolower)
strwrap(corpus[[1]])
# 1. Let's change all the comments to lower case.
corpus = reviews$comments
summary(corpus)
head(corpus)
corpus
View(reviews)
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
