ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = 0,#as.numeric(ymd("2011-01-01")),
xmax = 5000, #as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
#annotate("text", x = ymd("2011-10-01"), y = 7800,
#         color = palette_light()[[1]], label = "Train Region") +
#annotate("text", x = ymd("2012-10-01"), y = 1550,
#         color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2012-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2013-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years)
ggplot(all_years)
library(tidyverse)
library(tidyquant)
library(timetk)
library(broom)
ggplot(all_years)
View(all_years)
ggplot2(all_years)
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2012-2015.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2015-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
#3 years forecast with actual data
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2015-12-31")),
ymin = 0, ymax = 14000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
annotate("text", x = ymd("2014-6-30"), y = 1550,
color = palette_light()[[1]], label = "Forecast Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
geom_point(aes(x = date, y = cnt), data = bikes_future,
alpha = 0.5, color = palette_light()[[2]]) +
geom_smooth(aes(x = date, y = cnt), data = bikes_future,
method = 'loess') +
labs(title = "Bikes Sharing Dataset: 3-Years Forecast", x = "") +
theme_tq()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2011-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
ggplot(all_years, aes(displ, hwy)) + geom_point()
ggplot(all_years, aes(x=date, y=cnt)) + geom_point()
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
all_years <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/2014-2015.csv")
# Select date and count
all_years <- all_years %>%
select(datetime, cnt) %>%
rename(date = datetime)
ggplot(all_years)
# Visualize data and training/testing regions
all_years %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2013-01-01")),
xmax = as.numeric(ymd("2014-12-31")),
ymin = 0, ymax = 50000,
fill = palette_light()[[4]], alpha = 0.01) +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Read
bikes <- read_csv("/Users/imbottlebird/Course/3.15.774/1.HW/Final_project/R/day.csv")
# Select date and count
bikes <- bikes %>%
select(dteday, cnt) %>%
rename(date = dteday)
# Visualize data and training/testing regions
bikes %>%
ggplot(aes(x = date, y = cnt)) +
geom_rect(xmin = as.numeric(ymd("2012-07-01")),
xmax = as.numeric(ymd("2013-01-01")),
ymin = 0, ymax = 10000,
fill = palette_light()[[4]], alpha = 0.01) +
annotate("text", x = ymd("2011-10-01"), y = 7800,
color = palette_light()[[1]], label = "Train Region") +
annotate("text", x = ymd("2012-10-01"), y = 1550,
color = palette_light()[[1]], label = "Test Region") +
geom_point(alpha = 0.5, color = palette_light()[[1]]) +
labs(title = "Bikes Sharing Dataset: Daily Scale", x = "") +
theme_tq()
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
test <- bikes %>%
filter(date >= ymd("2012-07-01"))
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Model using the augmented features
fit_lm <- lm(cnt ~ ., data = train_augmented)
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
# RMSE
sqrt(mean(fit_lm$residuals^2))
test_augmented <- test %>%
tk_augment_timeseries_signature()
test_augmented
yhat_test <- predict(fit_lm, newdata = test_augmented)
pred_test <- test %>%
add_column(yhat = yhat_test) %>%
mutate(.resid = cnt - yhat)
# Split into training and test sets
train <- bikes %>%
filter(date < ymd("2012-07-01"))
train
# Add time series signature
train_augmented <- train %>%
tk_augment_timeseries_signature()
train_augmented
# Visualize the residuals of training set
fit_lm %>%
augment() %>%
ggplot(aes(x = date, y = .resid)) +
geom_hline(yintercept = 0, color = "red") +
geom_point(color = palette_light()[[1]], alpha = 0.5) +
theme_tq() +
labs(title = "Training Set: lm() Model Residuals", x = "") +
scale_y_continuous(limits = c(-5000, 5000))
summary(fit_lm)
test_augmented
corpus[[1]]   # individual doc
# 1. Let's change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array of document
library(caret)
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
library(caret)
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
setwd("~/Google Drive/Colab Notebooks/R/6.text_analytics-cart")
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
#a) Preliminary insights
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
# 1. Let's change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array of document
# install.packages("tm")
# install.packages("SnowballC")
library(tidyverse)
library(tm) # Will use to create corpus and modify text therein.
library(SnowballC) # Will use for "stemming."
library(rpart) # Will use to construct a CART model.
library(rpart.plot) # Will use to plot CART tree.
library(caret)
# 1. Let's change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array of document
corpus[[1]]   # individual doc
strwrap(corpus[[1]])
strwrap(corpus[[3]])
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
View(corpus)
# We now transform a link of the form "https://link" into "http link" to make sure http appears as a word
# We first define a new function f from gsub (substitute text):
# f is essentially the same as gsub except for the order of arguments
# gsub() replaces all matches of a strong. ex) gsub(targettext, newtext, stringvector)
f <- content_transformer(function(doc, oldtext, newtext) gsub(oldtext, newtext, doc))
corpus <- tm_map(corpus, f, "https://", "http ")  # f(each doc in corpus, "https://", "http ")
strwrap(corpus[[3]])
grep("https://", corpus)
strwrap(corpus[[2]])
strwrap(corpus[[1]])
View(corpus)
freq_terms(corpus, 3)
library(gdap)
?gdap
??gdap
library(quanteda)
require(quanteda)
devtools::install_github("kbenoit/quanteda", quiet = TRUE)
text_locate(corpus, 'ernest')
library('corpus')
termFreq(corpus)
termFreq(corpus, control=list("https://"))
# 2. we remove punctuation from the document
corpus <- tm_map(corpus, removePunctuation)
strwrap(corpus[[1]])
# 3. remove all stop words:
corpus = tm_map(corpus, removeWords, stopwords("english"))  # removeWords(corpus,stopwords("english"))
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[1]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[4]])
corpus = tm_map(corpus, removeWords, c("airbnb"))
strwrap(corpus[[4]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[1]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[2]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[3]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[5]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[6]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
strwrap(corpus[[7]])
plot(cars)
# 3. remove all stop words:
corpus = tm_map(corpus, removeWords, stopwords("english"))  # removeWords(corpus,stopwords("english"))
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[1]])
# Checking again:
strwrap(corpus[[2]])
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[2]])
reviews = read.csv("data/airbnb-small.csv", stringsAsFactors = FALSE)
#a) Preliminary insights
table(reviews$review_scores_rating)
reviews$nchar = nchar(reviews$comments)
aggregate(reviews$nchar, list(reviews$review_scores_rating), mean)
# 1. Let's change all the comments to lower case.
corpus = Corpus(VectorSource(reviews$comments)) # An array of document
corpus[[1]]   # individual doc
strwrap(corpus[[1]])
strwrap(corpus[[3]])
strwrap(corpus[[2]])
strwrap(corpus[[3]])
strwrap(corpus[[4]])
?inspect()
inspect(corpus[1:3])
inspect(corpus[,grep('airbnb')])
inspect(corpus[,grepl('airbnb')])
inspect(corpus[grepl('airbnb')])
inspect(grepl('airbnb',corpus))
grepl('airbnb',corpus)
dtm <- DocumentTermMatrix(corpus)
inspect(dtm[, grepl("airbnb", corpus)])
inspect(corpus)
grepl('airbnb',corpus)
??quanteda
?quanteda
install.packages("quanteda")
kwic(corpus, "airbnb")
?kwic
??kwic
library("quanteda")
kwic(corpus, "airbnb")
kwic(inspect(corpus), "airbnb")
reviews$comments
kwic(reviews$comments, "airbnb")
kwic(reviews$comments, pattern="airbnb")
strwrap(corpus[[5]])
strwrap(corpus[[6]])
strwrap(corpus[[15]])
strwrap(corpus[[29]])
strwrap(corpus[[162]])
# Checking again:
strwrap(corpus[[2]])
# The function tm_map applies an operation to every document in the corpus.
# In this case, the operation is 'tolower" (i.e. to lowercase).
corpus = tm_map(corpus, tolower)
strwrap(corpus[[1]])
corpus[[1]]   # individual doc
# 2. remove punctuation from the document
corpus <- tm_map(corpus, removePunctuation)
strwrap(corpus[[1]])
# 3. remove all stop words:
corpus = tm_map(corpus, removeWords, stopwords("english"))  # removeWords(corpus,stopwords("english"))
# stopwords("english") is a dataframe that constains a list of
# stop words. Let us look at the first ten stop words.
stopwords("english")[1:10]
# Checking again:
strwrap(corpus[[2]])
# Checking again:
strwrap(corpus[[1]])
# Checking again:
strwrap(corpus[[1]])
strwrap(corpus[[1]])
# Next, we remove the particular word: 'airbnb'
# This list can be customized depending on the application context
#kwic(reviews$comments, pattern="airbnb")
strwrap(corpus[[162]])
corpus = tm_map(corpus, removeWords, c("airbnb"))
strwrap(corpus[[162]])
# 3. Now we stem our documents. Recall that this corresponds toremoving the parts of words
# that are in some sense not necessary (e.g. 'ing' and 'ed').
corpus = tm_map(corpus, stemDocument)
# We have:
strwrap(corpus[[1]])
# 4. Let us "sparsify" the corpus and remove infrequent words.
# First, we calculate the frequency of each words over all tweets.
frequencies = DocumentTermMatrix(corpus)
frequencies                              # documents as the rows, terms as the columns
frequencies   # documents as the rows, terms as the columns
# Let us get a feel for what words occur the most. Words that appear at least 900 times:
findFreqTerms(frequencies, lowfreq=900)
# Let us only keep terms that appear in at least 1% of the tweets and
# create a list of these words as follows.
sparse = removeSparseTerms(frequencies, 0.99)  # 0.99: maximal allowed sparsity
sparse # We now have 172 terms instead of 12,093
### d) Training and Test Split
# We first create a new data frame.
# Each column corresponds to each word (term),
# and each row corresponds to each document (review)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms)
# Lastly, we create a new column for the dependent variable:
document_terms$positive_review = reviews$review_scores_rating >= 80
str(document_terms)
# We have processed our data! Let us briefly construct a CART model.
head(reviews)
# We have processed our data! Let us briefly construct a CART model.
head(document_terms)
# We have processed our data! Let us briefly construct a CART model.
summary(reviews)
# We have processed our data! Let us briefly construct a CART model.
summary(reviews)
# We have processed our data! Let us briefly construct a CART model.
head(reviews)
# Training and test set.
split1 = (reviews$data < "2017-12-31")
split2 = (reviews$data >= "2018-01-01")
train = document_terms[split1,]
test = document_terms[split2,]
### e) Prediction
# Constructing and plotting the CART model.
# classification
cart = rpart(psotive_review ~ ., data=train, method="class", cp = .003)
### e) Prediction
# Constructing and plotting the CART model.
# classification
cart = rpart(positive_review ~ ., data=train, method="class", cp = .003)
prp(cart)
### e) Prediction
# Constructing and plotting the CART model.
# classification
cart = rpart(positive_review ~ ., data=train, method="class")
reviews[grepl("word",reviews$comments),"comments"]
### e) Prediction
# Constructing and plotting the CART model.
train$
# classification
cart = rpart(positive_review ~ ., data=train, method="class")
### e) Prediction
# Constructing and plotting the CART model.
table(train)
train# classification
cart = rpart(positive_review ~ ., data=train, method="class")
### e) Prediction
# Constructing and plotting the CART model.
table(train)
### e) Prediction
# Constructing and plotting the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
### e) Prediction
# Constructing and plotting the CART model.
cart = rpart(positive_review ~ ., data=train, method="class", cp=0.01)
View(reviews)
View(document_terms)
# Lastly, we create a new column for the dependent variable:
document_terms$positive_review = reviews$review_scores_rating >= 80
View(document_terms)
# Lastly, we create a new column for the dependent variable:
document_terms$positive = reviews$review_scores_rating >= 80
View(document_terms)
document_terms$positive <- NULL
View(train)
### e) Prediction
# Constructing and plotting the CART model.
cart = rpart(positive_review ~ ., data=document_terms, method="class", cp=0.01)
prp(cart)
### e) Prediction
# Constructing and plotting the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
View(test)
# Lastly, we create a new column for the dependent variable:
document_terms$review_length = nchar(reviews$comments)
# Training and test set.
split1 = (reviews$data < "2018-01-01")
split2 = (reviews$data >= "2018-01-01")
train = document_terms[split1,]
test = document_terms[split2,]
### e) Prediction
# Constructing and plotting the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
# Training and test set.
split1 = (reviews$date < "2018-01-01")
split2 = (reviews$date >= "2018-01-01")
train = document_terms[split1,]
test = document_terms[split2,]
document_terms$review_length <- NULL
# Training and test set.
split1 = (reviews$date < "2018-01-01")
split2 = (reviews$date >= "2018-01-01")
train = document_terms[split1,]
test = document_terms[split2,]
### e) Prediction
# Constructing and plotting the CART model.
cart = rpart(positive_review ~ ., data=train, method="class")
prp(cart)
# Assessing the out-of-sample performance of the CART model
predictions.cart <- predict(cart, newdata=test, type="class")
matrix.cart = table(test$TrumpWrote, predictions.cart) # confusion matrix
matrix.cart = table(test$positive_review, predictions.cart) # confusion matrix
accuracy.cart = (matrix.cart[1,1]+matrix.cart[2,2])/nrow(test)
TPR.cart = (matrix.cart[2,2])/sum(matrix.cart[2,])
FPR.cart = (matrix.cart[1,2])/sum(matrix.cart[1,])
